# Server configuration
FASTIFY_PORT=3030

# LangGraph agent configuration
# LANGMEM_HOTPATH_LIMIT controls how many recent raw messages stay verbatim before summarization.
# LANGMEM_HOTPATH_TOKEN_BUDGET (tokens) caps the size of that window across messages.
# LANGMEM_RECENT_MESSAGE_FLOOR guarantees a minimum number of latest messages remain unsummarized.
# LANGMEM_HOTPATH_MARGIN_PCT keeps headroom within the token budget (e.g. 0.1 = use at most 90%).
LANGGRAPH_SYSTEM_PROMPT="You are Cerebrobot, a helpful assistant."
LANGGRAPH_PERSONA_TAG="operator"
LANGCHAIN_MODEL="gpt-4o-mini"
LANGCHAIN_TEMPERATURE=0.7
LANGMEM_HOTPATH_LIMIT=10
LANGMEM_HOTPATH_TOKEN_BUDGET=3000
LANGMEM_RECENT_MESSAGE_FLOOR=4
LANGMEM_HOTPATH_MARGIN_PCT=0.1

# DeepInfra configuration (for LLM and embeddings)
DEEPINFRA_API_KEY="your-deepinfra-api-key"
DEEPINFRA_API_BASE="https://api.deepinfra.com/v1/openai"

# Long-term memory configuration (Phase 2 - LangGraph Store)
# Note: Embedding dimensions (1536) are hardcoded to match Qwen/Qwen3-Embedding-8B
# If switching models, update EMBEDDING_DIMENSIONS in apps/server/src/agent/memory/config.ts
MEMORY_EMBEDDING_ENDPOINT="https://api.deepinfra.com/v1/openai"
MEMORY_EMBEDDING_MODEL="Qwen/Qwen3-Embedding-8B"
MEMORY_SIMILARITY_THRESHOLD=0.7
MEMORY_MAX_TOKENS=2048
MEMORY_INJECTION_BUDGET=1000
MEMORY_RETRIEVAL_TIMEOUT_MS=5000

# Postgres persistence (Phase 1.5)
POSTGRES_USER=cerebrobot
POSTGRES_PASSWORD=cerebrobot
POSTGRES_DB=cerebrobot
POSTGRES_PORT=5432
DATABASE_URL="postgresql://cerebrobot:cerebrobot@localhost:5432/cerebrobot"

# LangGraph Postgres checkpoint configuration
# Point to localhost when running the backend directly, or to the docker-compose
# network host `postgres` from inside containers/scripts.
LANGGRAPH_PG_URL="postgresql://cerebrobot:cerebrobot@localhost:5432/cerebrobot"

# Frontend configuration
CLIENT_PORT=5173
VITE_API_BASE="http://localhost:3030"
