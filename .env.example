# Server configuration
FASTIFY_PORT=3030

# LangGraph agent configuration
# LANGMEM_HOTPATH_LIMIT controls how many recent raw messages stay verbatim before summarization.
# LANGMEM_HOTPATH_TOKEN_BUDGET (tokens) caps the size of that window across messages.
# LANGMEM_RECENT_MESSAGE_FLOOR guarantees a minimum number of latest messages remain unsummarized.
# LANGMEM_HOTPATH_MARGIN_PCT keeps headroom within the token budget (e.g. 0.1 = use at most 90%).
LANGGRAPH_SYSTEM_PROMPT="You are Cerebrobot, a helpful assistant."
LANGGRAPH_PERSONA_TAG="operator"
LANGCHAIN_MODEL="gpt-4o-mini"
LANGCHAIN_TEMPERATURE=0.7
LANGMEM_HOTPATH_LIMIT=10
LANGMEM_HOTPATH_TOKEN_BUDGET=3000
LANGMEM_RECENT_MESSAGE_FLOOR=4
LANGMEM_HOTPATH_MARGIN_PCT=0.1

# OpenAI-compatible provider (required for LangGraph agent)
OPENAI_API_KEY="your-deepinfra-or-openai-key"
OPENAI_API_BASE="https://api.deepinfra.com/v1/openai"

# Postgres persistence (Phase 1.5)
POSTGRES_USER=cerebrobot
POSTGRES_PASSWORD=cerebrobot
POSTGRES_DB=cerebrobot
POSTGRES_PORT=5432
DATABASE_URL="postgresql://cerebrobot:cerebrobot@localhost:5432/cerebrobot"

# LangGraph Postgres checkpoint configuration
# Point to localhost when running the backend directly, or to the docker-compose
# network host `postgres` from inside containers/scripts.
LANGGRAPH_PG_URL="postgresql://cerebrobot:cerebrobot@localhost:5432/cerebrobot"

# Frontend configuration
CLIENT_PORT=5173
VITE_API_BASE="http://localhost:3030"
